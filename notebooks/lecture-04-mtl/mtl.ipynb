{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-task learning (MTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know how you can use data from other tasks (or views) \n",
    "* understand the basic principles behind multi-task learning (MTL)\n",
    "* [be able to implement a simple MTL example in Keras] // most probably no time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge: \n",
    "\n",
    "How can we hope to use **data from other tasks** where **both the input and output spaces are different**? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1: Card game\n",
    "\n",
    "You are in beautiful Italy and want to get acquainted with local card games. You hear about 'scala 40', and are eager to learn it. \n",
    "\n",
    "The input space are cards, and the output space are configuations (hands) of your cards. You seem acquainted with the type of input space, but the rules have changed. Luckily you know already how to play poker. Rather than starting from scratch (*tabula rasa*), you use your internal knowledge of poker (or generally how to play a card game) to learn how to play 'scala 40', you can quicker get what are valiable configurations (possible outputs) for the new game.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Royal_straight_flush.jpg/1920px-Royal_straight_flush.jpg\" width=300> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2: riding a motorcycle\n",
    "\n",
    "We have seen this example already during day 1. You want to learn how to ride a motorcycle, the input space is the street, the output space the possible actions you can take, accelaterate, break, change gears, etc. \n",
    "\n",
    "Some skills are unique to driving a motorcycle (need for hand for the clutch, to worry not to tip over when stopping, etc). However, you can use your internal knowledge of how to drive a car in order to learn how to drive a motorcycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-task learning (MTL)\n",
    "\n",
    "\n",
    "##### Single neural networks\n",
    "The figure shows three **separate** feedforward neural networks for three different tasks.\n",
    "\n",
    "<img src=\"pics/stl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea of **multi-task learning** (Caruana, 1997; Collobert et al., 2011) to exploit the training signal of **other tasks**. \n",
    "\n",
    "But before we go into MTL, lets recap learning for a single task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Backpropagation\n",
    "\n",
    "### Learning a single tasks\n",
    "\n",
    "Link to [Notebook Recap learning](Learning-recap.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-task learning\n",
    "\n",
    "We have seen the computational graph abstraction and backpropagation for a single task. \n",
    "\n",
    "A common approach to multi-task learning is to **add additional output layers** to a neural network that otherwise shares the same underlying structure.\n",
    "\n",
    "MTL does exploit other tasks by **learning multiple tasks** in parallel while  **using a shared structure/representation**.\n",
    "\n",
    "<img src=\"pics/mtl.png\" width=300>\n",
    "\n",
    "By sharing representations a model is train **jointly** for both/all tasks. \n",
    "\n",
    "### Why is this useful?\n",
    "\n",
    "We can use MTL when we believe that information useful for one type of prediction can also be useful for another type (tasks). \n",
    "\n",
    "Instead of creating separate networks for each task, we can build a single network with shared layers. By using shared representations most **parameters are shared** between the tasks, and information learned to be useful for one task might also be useful for other tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The computational graph makes it very easy to compute separate losses for each task. These losses are then summed and backpropagated through the network. \n",
    "\n",
    "<img src=\"pics/mtl-loss.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that there can be different specific setups:\n",
    "\n",
    "#### Same corpus labeled for different tasks (jointly labeled data)\n",
    "\n",
    "In this case you have the same input but several different tasks annotated on top. For example, you might have a corpus of sentences annotated for both part-of-speech tags and named entities. \n",
    "\n",
    "A MTL here could be to train a single network that outputs POS and NER labels in two different output nodes.\n",
    "\n",
    "#### Several corpora (distincly labeled data)\n",
    "\n",
    "However, an advantage of MTL in neural networks is that you do not need to have jointly labeled data, your datasets might also come from different sources. \n",
    "\n",
    "In this case the training procedure will take the input from the different corpora and perform gradient computations with respect to the particular loss function (and task) of the current training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Does MTL work? \n",
    "\n",
    "[lets look at an example (freqbin)] to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Example with distinct sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Why does it work? possible explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### What are related tasks? "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
