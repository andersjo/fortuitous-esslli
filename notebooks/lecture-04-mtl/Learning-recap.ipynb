{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Graph view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After this notebook you should:\n",
    "* know about the different equivalent formulizations of a neural network \n",
    "* understand the computational graph abstraction and how it relates to backprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A neural network can be formalized, both algebraically and graphically. we can think of a feed-forward NN as a function $NN(\\mathbf{x})$: $$y= NN(\\mathbf{x}) $$\n",
    "\n",
    "with:\n",
    "input: $\\mathbf{x}$ (vector with $d_{in}$ dimensions)\n",
    "\n",
    "output: $\\mathbf{y}$ (output with $d_{out}$ classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, a simple feedforward neural network can be visualized (and formalized) as:\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Where do the weights come frome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's an **optimization** problem. We want to find the weights that \"work best\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/mountains_at_home.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training a Neural Network: Ingredients\n",
    "\n",
    "* we need to **define what \"works best\" means**\n",
    "* we need **a way to change the model (parameters)** to get closer to a good model (hint: SGD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining what works best ~ how close we are: Loss\n",
    "\n",
    "Measures how 'far off' we are from true solution:\n",
    "\n",
    "$$L(\\mathbf{\\hat{y}},\\mathbf{y})$$\n",
    "\n",
    "For multi-class classification the **cross-entropy** is a commonly used loss function: \n",
    "\n",
    "$$L_{crossentropy}(\\mathbf{\\hat{y}},\\mathbf{y})= - log(\\hat{y}_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to get closer to a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 1:** random guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 2:** start with some random initial parameters (weights), and randomly adjust them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 3:** follow the gradient: analytical method to find the best direction along which we should change our weight vector: **gradient descent** (see first days of this class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6d/Error_surface_of_a_linear_neuron_with_two_input_weights.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To sum up: Ingredients for training a Neural Network\n",
    "\n",
    "* we need to define what \"works best\" means \n",
    "    $\\rightarrow$ minimize some **loss**\n",
    "* we need a way to change the parameters to get closer to a good model\n",
    "    $\\rightarrow$ **minimize loss using a gradient-based method: gradient descent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Intuitively, training a neural networks involves the following steps:\n",
    "\n",
    "* compute the gradient of the loss function with respect to the parameters\n",
    "* move the parameters in the negative direction of the gradient to decrease the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Skeleton of gradient descent:\n",
    "    \n",
    "**Input**: training set, loss function $L$\n",
    "\n",
    "Repeat for number of iterations (**epochs**): \n",
    " \n",
    "* compute loss on data: $L(X,Y)$\n",
    "* compute gradients: $\\mathbf{g} = L(X,Y)$ with respect to $w$\n",
    "* move parameters in direction of the negative gradient: $w \\pm -\\eta \\mathbf{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to get an intuitive understanding of the **backpropagation** algorithm. Backprob is a way of computing **gradients** of expressions through applying the **chain rule**. A powerful way to compute these gradients is to see the network as a  **computational graph**.\n",
    "\n",
    "\n",
    "#### Why computational graph?\n",
    "\n",
    "It helps us to understand the flow in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### In a computational graph:\n",
    "* nodes are operations\n",
    "* gray boxes are parameters\n",
    "\n",
    "The neural network from before:\n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "is represented as computational graph:\n",
    "\n",
    "<img src=\"pics/compgraph1.png\" alt=\"See Goldberg (2015) primer for more details\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Complete computational graph\n",
    "\n",
    "The computational graph from before is not complete. It has no input and no output. A complete graph needs to define how the **input** looks like, and needs to specify a loss.\n",
    "\n",
    "* loss: $L(\\mathbf{\\hat{y}},\\mathbf{y})$\n",
    "* input: 4 words embedded into a 50-dimensional word embedding space and *concatenated* \n",
    "<img src=\"pics/compgraphcomplete.png\">\n",
    "\n",
    "Why is the computational graph handy?\n",
    "\n",
    "* It allows us to easily compute the predictions of a network from the input in a **forward pass** (follow the arrows)\n",
    "* In the **backward pass** (derivatives, backprop) the computational graph helps to compute the gradients for the parameters with respect to a specific scalar output loss. This is what most deep learning toolkits use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To recap, when working with a neural network\n",
    "You need to:\n",
    "\n",
    "* to specify how the model looks like (e.g., feedforward, RNN, etc.)\n",
    "* define a loss function (e.g., crossentropy, cosine)\n",
    "* pick an optimization procedure (e.g. SGD, Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 6: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
