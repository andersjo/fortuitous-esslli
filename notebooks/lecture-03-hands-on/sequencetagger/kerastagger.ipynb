{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type-constraint POS tagging in Keras\n",
    "\n",
    "In this exercise you will learn how to implement a sequence prediction model in Keras and extend the tagger to use type-constraint decoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple POS tagger based on Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging is the task of assigning a part-of-speech (word class) to every input token. \n",
    "For example:\n",
    "\n",
    "`POS/NOUN tagging/NOUN is/VERB cool/ADJ`\n",
    "\n",
    "There exist different POS tagsets. They differ in type and granularity of the POS tag inventory. \n",
    "\n",
    "In this exercise we will use the Google universal tagset [(Petrov et al., 2012)](www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf), which consists of 12 POS classes: ``NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), ‘.’ (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).``\n",
    "\n",
    "\n",
    "### A RNN-based POS tagger\n",
    "\n",
    "We will build a simple POS tagger based on a Recurrent Neural Network, which you have learned about yesterday. In fact, RNNs can come in many different flavors (illustration by [Karpathy](http://karpathy.github.io/assets/rnn/diags.jpeg)):\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\">\n",
    "\n",
    "For our tagging scenario, we want to have a model that outputs a tag for every input token:\n",
    "\n",
    "\n",
    "<img src=\"pics/karpathy-rnn.png\" width=150>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a neural network library in Python that supports both `theano` and `tensorflow`.\n",
    "\n",
    "Keras implements two approaches:\n",
    "\n",
    "* the Sequential model\n",
    "* the functional API\n",
    "\n",
    "The core data structure in Keras is the `Sequential` model, a linear stack of layers. If you want to implement more complex model, then go with the `functional API`. More details: [tiny intro Keras](https://github.com/bplank/ltp-notebooks/blob/master/mini-intro-Keras.ipynb), and [30 seconds into Keras](https://keras.io/).\n",
    "\n",
    "Lets start of from the Sequential model. For the POS tagger, we have prepared a basic skeleton for you. Lets import this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.tagger import SequenceTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic abstract class that implements already some functionality, like input/output handling (`read_data`), and the class provides basic `fit` and `predict` functions to train a model and test it on new data. However, the class itself is a bare skeleton, it does know yet know how the model itself looks like. \n",
    "\n",
    "So our first task is to specify the model structure. We create our own subclass `BasicSequenceTagger` from the basic `SequenceTagger` class, and specify the **model** by implementing your own `build_graph` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "class BasicSequenceTagger(SequenceTagger):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.in_dim=64\n",
    "        self.h_dim=100\n",
    "\n",
    "    def build_graph(self):\n",
    "        ### version1: traditional 'keras'-style code\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.get_num_features(), self.in_dim,\n",
    "                                 input_length=self.max_sentence_len, mask_zero=True))\n",
    "        self.model.add(LSTM(self.h_dim, return_sequences=True))\n",
    "        self.model.add(TimeDistributed(Dense(self.get_num_tags())))\n",
    "        self.model.add(Activation('softmax'))\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your time to go through the code. Discuss with your neighbor: \n",
    "* How do the parts connect to the illustration above?\n",
    "* What does `return_sequences=True` mean? Why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the tagger\n",
    "\n",
    "Lets train the tagger. We provide you a sample dataset (from the Universal dependencies project, but converted to the 12 POS tags). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/en-ud-train-5000-12.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000 sentences 82500 tokens\n",
      "11998 features\n",
      "max_sentence_len: 104\n"
     ]
    }
   ],
   "source": [
    "# create tagger and read data\n",
    "tagger = BasicSequenceTagger()\n",
    "tagger.read_data(\"data/en-ud-train-5000-12.conll\", \"data/en-ud-test-200-12.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model\n",
      "train..\n",
      "Epoch 1/6\n",
      "5000/5000 [==============================] - 25s - loss: 2.0572 - acc: 0.0640    \n",
      "Epoch 2/6\n",
      "5000/5000 [==============================] - 22s - loss: 0.7979 - acc: 0.1259    \n",
      "Epoch 3/6\n",
      "5000/5000 [==============================] - 23s - loss: 0.3511 - acc: 0.1440    \n",
      "Epoch 4/6\n",
      "5000/5000 [==============================] - 24s - loss: 0.2126 - acc: 0.1501    \n",
      "Epoch 5/6\n",
      "5000/5000 [==============================] - 23s - loss: 0.1500 - acc: 0.1526    \n",
      "Epoch 6/6\n",
      "5000/5000 [==============================] - 24s - loss: 0.1130 - acc: 0.1540    \n"
     ]
    }
   ],
   "source": [
    "print(\"Build model\")\n",
    "tagger.build_graph()\n",
    "print(\"train..\")\n",
    "batch_size=50\n",
    "epochs=6\n",
    "tagger.fit(batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "correct 5174.0 total: 5715.0 accuracy: 0.9053368328958881\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "tagger.predict(tagger.test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Test the tagger with different number of epochs. What do you observe?\n",
    "\n",
    "A nice way to visualize the model is the `model.summary()` function in Keras. This will give you more information on the model input/outputs and number of parameters. Try to add it to the tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type constraints\n",
    "\n",
    "The idea behind type-constraint is to exploit additional information to guide your model. \n",
    "\n",
    "Say, someone told you that every sentence starts with a noun. You could restrict your model to only output nouns at sentence-initial positions. This is obviously too restrictive and not true. But this is the basic idea behind type constraints. \n",
    "\n",
    "Note, this technique is called **type** constraints. This already alludes at the idea of using a dictionary as additional, fortuitous data. A dictionary lists the possible word classes for a given word type. More formally, a word type $w \\in \\mathcal{V}$ is mapped to a set of admissible tags $\\mathcal{Y}(w) \\subseteq \\mathcal{Y}$. For word types that are not in the dictionary, we allow all possible tags, i.e., $\\mathcal{Y}$.\n",
    "\n",
    "Now, using these constraints for words in the dictionary we restrict the tagger to output only tags that are allowed.\n",
    "\n",
    "Here is an example (from [Tackström et al., (2013)](http://soda.swedish-ict.se/5472/1/paper1.pdf)):\n",
    "\n",
    "<img src=\"pics/typeconstr.png\">\n",
    "\n",
    "For instance, the first word in the example sentence is not in the dictionary, hence all tags are allowed. The remaining words are in the dictionary so only their admissible subset $\\mathcal{Y}(w)$ is shown.\n",
    "\n",
    "\n",
    "### Exercise: \n",
    "\n",
    "Add type-constraint inference to the tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "class BasicSequenceTaggerTypeConstraint(SequenceTagger):\n",
    "    \n",
    "    def __init__(self, wiktionaryfile):\n",
    "        self.w2i = {} # mapping word to word indices\n",
    "        self.t2i = {} # mapping tag to tag indices\n",
    "        self.in_dim=64\n",
    "        self.h_dim=100\n",
    "        self.word2tags = self.read_wiktionary(wiktionaryfile)\n",
    "        \n",
    "    def read_wiktionary(self, wiktionaryfile):\n",
    "        pass #TODO\n",
    "\n",
    "    def build_graph(self):\n",
    "        ### version1: traditional 'keras'-style code\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.get_num_features(), self.in_dim,\n",
    "                                 input_length=self.max_sentence_len, mask_zero=True))\n",
    "        self.model.add(LSTM(self.h_dim, return_sequences=True))\n",
    "        self.model.add(TimeDistributed(Dense(self.get_num_tags())))\n",
    "        self.model.add(Activation('softmax'))\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    def predict(self, input, output=None, batch_size=32):\n",
    "        # This is the standard predict method (copy from SequenceTagger)\n",
    "        # probs is a tensor (num_instance, max_sentence_len, number_classes)\n",
    "        # note that it is padded (pre-padded with zeros)\n",
    "        probs = self.model.predict(input, batch_size=batch_size)\n",
    "        predictions = probs.argmax(axis=-1)\n",
    "        return self.evaluate(predictions, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Read in Wiktionary\n",
    "\n",
    "Add the necessary code to the `read_wiktionary` function. The dictionary is a plain text file in the format `word<tab>tag`, with a single tag per line. \n",
    "\n",
    "Once you have your code in place you can examine `word2tags`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger = BasicSequenceTaggerTypeConstraint(\"data/small.dic\")\n",
    "tagger.word2tags['enough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Add type-constraint inference\n",
    "  \n",
    "Now that you have the dictionary in place, how can you implement these restrictions? \n",
    "Which parts of the tagger do you need to modify?\n",
    "\n",
    "We need to instruct the tagger to:\n",
    "\n",
    "* allow only tags that are licensed by the dictionary,\n",
    "* otherwise put no restrictions on the possible output.\n",
    "\n",
    "Add the necessary code to the tagger above. Then train and test the model with type constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"read data\")\n",
    "tagger.read_data(\"data/en-ud-train-5000-12.conll\", \"data/en-ud-test-200-12.conll\")\n",
    "print(\"Build model\")\n",
    "tagger.build_graph()\n",
    "print(\"train..\")\n",
    "batch_size=50\n",
    "epochs=6\n",
    "tagger.fit(batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get type-constraint predictions\n",
    "tagger.predict(tagger.test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiktionary\n",
    "\n",
    "A popular community-created dictionary is [Wiktionary](https://en.wiktionary.org/wiki/Wiktionary:Main_Page). \n",
    "It is frequently used in NLP, both for type-constraint decoding or evaluation, cf. Li et al., (2012), Tackström et al., (2013).\n",
    "\n",
    "You can download the dictionaries derived from Wiktionary by Li et al. [here](https://code.google.com/archive/p/wikily-supervised-pos-tagger/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Extra exercises:\n",
    "\n",
    "* Another way to make the system more robust is to use *dropout* (Hinton et al., 2012). Check the Keras documentation. Add dropout to your tagger.\n",
    "\n",
    "* For more advanced models the functional API is more appropriate. Translate the code above into functional API code (i.e., every layer is now a function, and you can give names to layers). Here is how the first two lines of code would look like (you now need to define an explicit `Input` layer):\n",
    "\n",
    "`\n",
    "input_words = Input(batch_shape=(args.batch_size, self.max_sentence_len,),\n",
    "                            dtype='int32', name='word_input')\n",
    "word_embeddings = Embedding(self.get_num_features(), args.in_dim,\n",
    "                                    input_length=self.max_sentence_len,\n",
    "                                    mask_zero=True)(input_words)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "* [Li et al., (2012)](http://www.seas.upenn.edu/~taskar/pubs/wikipos_emnlp12.pdf)\n",
    "* [Tackström et al., (2013)](http://soda.swedish-ict.se/5472/1/paper1.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
