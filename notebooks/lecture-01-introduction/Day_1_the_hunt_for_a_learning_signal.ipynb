{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improving language technology with fortuitous data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) is \"is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. [..] challenges in NLP [include] **enabling computers to derive meaning from human or natural language input**\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Modern NLP algorithms are based on **machine learning**\" ([NLP, Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n",
    "<img src=\"pics/prog-vs-ml.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus mainly on *supervised learning*, where the experience are pairs $(x,y)$ called **training examples**. Each $x$ is an example from some **input space** $\\mathcal{I}$, e.g., an image, a piece of text,.. Hence, $x$ is the system's input, and $y$ the desired output. For example, in natural language processing (NLP) $x$ could be a tweet and $y$ could be its sentiment, or $x$ could be a sentence and $y$ is syntactic parse tree; and so forth.\n",
    "\n",
    "Machine learning can be see as inducing a **function** $f(\\cdot)$ that maps examples from an **input space** to an **output space** $\\mathcal{O}$:\n",
    "\n",
    "$$f(\\cdot) : \\mathcal{I} \\rightarrow \\mathcal{O}$$\n",
    "\n",
    "<img src=\"pics/inputspace.png\" width=500>\n",
    "\n",
    "In Natural Language Processing, we are mostly interested in learning **discrete** output spaces. Our output space will usually consist of a discrete set of **labels**  $\\mathcal{Y}$. For example, in sentiment analysis the output space could be represented as $\\mathcal{Y} = \\{positive, negative\\}$. The output might also be structured, for instance, $y$ can be parse tree of a sentence. We will see examples of structured input and output in lecture 2. For now, lets define the **input** $\\mathcal{X}$ as a set of discrete objects, e.g., the words in a document. \n",
    "\n",
    "In machine learning, we usually have access to a **finite** sample of the input-output space, generated by some underlying distribtion $\\mathcal{D}$. We do not have access to the distribution that generated the data, only the finite sample that it came from. In supervised learning, this finite sample constitutes our **labeled training examples**, i.e., $L = \\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$. In unsupervised learning, we only have unlabeled examples, $U = \\{(x_1),\\ldots,(x_n)\\}$. However, lets focus on supervised learning for now.\n",
    "\n",
    "A trivial way to define the function $f(\\cdot)$ would be to just *memorize* the training data. \n",
    "\n",
    "However, ultimately we are interested in a system that is able to **generalize**, i.e., that provides reasonable outputs even for examples the system hasn't seen before. \n",
    "\n",
    "Thus, a system is usually **evaluated** in terms of how well it does on possibly **unseen data**. Specifically, given a new input, the system gets as input $x$ and makes a prediction $\\hat{y}$ (**predicted label**). The system incurs a **loss** (or cost) $l(y,\\hat{y})$ which is typically $0$ if the predicted label is correct, and $>0$ otherwise (if $y\\ne \\hat{y}$).  \n",
    "\n",
    "Our trivial system that just memorizes the training data thus **fails to generalize**. It simply does not know what to do with an example it has not seen before. \n",
    "\n",
    "Generalization thus means going beyond memorization. We want a learner that is not dumb. One crucial observation is that our examples are usually not atomic units, but are object that can be decomposed into **features**, thus are typically represented in some **feature space** $\\mathcal{Z}$. Even though the learner might not have seen the new example exactly, it might have seen similar examples (or parts of the current example), and thus still be able to make a prediction.\n",
    "\n",
    "Specifically, each input example is transformed into a suitable **representation** for the learning algorithm by a **feature function** $\\phi(x)$. The feature function $\\phi(\\cdot)$ maps examples from the input space to the internal feature space:\n",
    "\n",
    "$$\\phi: \\mathcal{X} \\rightarrow \\mathcal{Z}$$\n",
    "\n",
    "To recap, the **goal** of a machine learning system is then to learn a function that maps our inputs to outputs:\n",
    "\n",
    "$$f(\\cdot) : \\mathcal{I} \\rightarrow \\mathcal{O}$$\n",
    "\n",
    "where our input and outputs are defined by the instance and label space $\\mathcal{X}, \\mathcal{Y}$.\n",
    "\n",
    "We here define two concrete steps that are involved when learning to map inputs to output. Specifically, $f(\\cdot)$ is composed of:\n",
    "\n",
    "$$f(\\cdot) : \\phi \\circ \\zeta $$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\phi: \\mathcal{X} \\rightarrow \\mathcal{Z}$$\n",
    "and\n",
    "$$\\zeta: (\\mathcal{Z},\\theta) \\rightarrow \\mathcal{Y}$$\n",
    "i.e.,\n",
    "\n",
    "* a function $\\phi$ that maps inputs to feature representation; and\n",
    "* a function $\\zeta$ that maps from the feature space to the label space, using an additional input, the learned weights $\\theta$ (they are set during the learning procedure); the scoring function is closely related to the learning algorithm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our machine learning system has three integral pieces:\n",
    "\n",
    "* data and data representation:\n",
    "    * the available data: labeled $L$\n",
    "    * the feature representation: $\\phi(X)$:\n",
    "\n",
    "* an algorithm for optimizing and objective function to determine the parameters $\\theta$: e.g., $SGD$, where the scoring function $\\zeta$ is used to map from feature representations to predicted labels $\\hat{y}$, and\n",
    "\n",
    "* an objective function/loss function: $\\mathcal{l}(y,\\hat{y})$ that gives us an estimate how good the current model, specified by $\\theta$ is\n",
    "\n",
    "The intuition of the algorithm is to set the model weights (parameters) $\\theta$ so that the loss of the model is minimized. (see more details later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the whole:\n",
    "\n",
    "<img src=\"pics/learning.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "* $\\mathcal{I}$: input space, $\\mathcal{O}$: output space\n",
    "* dataset $\\mathcal{D}$ consisting of $\\mathcal{X}$: input examples with labels in  $\\mathcal{Y}$: discrete label space \n",
    "* $(x,y)$: a concrete training example consisting of an input $x$ and desired output $y$, where $x \\in \\mathcal{X}$ and $ y \\in \\mathcal{Y}$\n",
    "* a feature function $\\phi(\\cdot)$ mapping an input $x$ to an internal representation or feature space $\\mathcal{Z}$\n",
    "* a scoring function $\\zeta(\\cdot,\\cdot)$ that takes two inputs: the feature representation $\\mathcal{Z}$ and a set of parameters $\\theta$ (weights) and maps the instance and weights to outputs in the label space $\\mathcal{Y}$ (technically, scoring function uses weights to calculate distribution over possible label, and then typically an argmax is done to get the predicted label); here I put all those steps into the function\n",
    "* an objective function (which usually corresponds to a concrete cost function that is minimized during learning on the training data, $l(y,\\hat{y})$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "There are two main challenges machine learning/NLP systems are faced with:\n",
    "\n",
    "* **wrong assumptions**: The underlying assumption of ML is that there should be a strong relationship between the data that our algorithm sees at training time and the data it sees at test time. This is almost never the case! \n",
    "\n",
    "* **limited samples**: There is never enough labeled data! In fact, $L$ is *tiny* compared to potential data out there. Why? Training data might be expensive or hard to collect. We cannot just simply annotate more data. Even worse, data changes continuously, it is not obvious to delimit *what* we want to annotate. And while we reach that point, our data might be already outdated. thus annotate what? \n",
    "\n",
    "<img src=\"pics/datapool.png\" width=500>\n",
    "\n",
    "We want to avoid going the manual data annotation route and instead use non-obvious sources of information that just waits to be harvested to build better models across different impoverished data situation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The continuum of impoverished data and fortuitous data to the rescue\n",
    "\n",
    "In NLP, data can differ in various ways (continuum of data differences), it might be that mostly the lexicon changes, e.g., when you go from book to DVD reviews in sentiment analysis, but it might go as far as being completely different languages. Say you have a parser trained on English, but now you want to get a parser for Icelandic, but you don't have any annotated data to start of with.\n",
    "\n",
    "\n",
    "Examples of impoverished data arise whenever we go to data situations that are non-standard/non-canonical:\n",
    "\n",
    "* processing non-standard data (going from newspaper to Twitter)\n",
    "* processing data in other languages (going to low-resource languages)\n",
    "\n",
    "<img src=\"pics/datadiff.png\" width=500>\n",
    "\n",
    "Related work here falls under the general umbrella of **transfer learning**, with particular instances being **domain adaptation** (learning across different domains), **cross-lingual learning** (where domains can be seen as different languages), and **multi-task learning** (where one learns from related tasks). However, the terms *domain/language* and *task* are fuzzy and it is not important here to make a hard distinction. Rather, we can all of those  research areas as related to the problem of impoverished data. \n",
    "\n",
    "Impoverished here means that we need to process data that is non-canonical. We here start from the prototypical canonical **newspaper English**. Simply because in NLP this has become the the-facto standard (most probably due to a historical accident). Nevertheless, lets take this historical fact as our reference point, and define our continuum of impoverished data with respect to deviations from newspaper English, which could be deviations in features, in labels or both, going to completely different languages or label distributions.\n",
    "\n",
    "#### What do do about impoverished data\n",
    "\n",
    "* annotate more data; problematic; plus annotate what?\n",
    "* normalize (make our data more similar to the canonical form); again problematic; normalize how? what defines norm?\n",
    "\n",
    "**Solution**: Harvest **data from non-obvious data sources**, i.e., **fortuitous data**, that can help our learner to better generalize to new unseen data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approaches\n",
    "\n",
    "big goal:\n",
    "$$f(\\cdot) : \\mathcal{I} \\rightarrow \\mathcal{O}$$\n",
    "\n",
    "approach taken:\n",
    "\n",
    "$$f(\\cdot) : \\phi \\circ \\zeta $$\n",
    "\n",
    "* modify first part: $\\phi: \\mathcal{X} \\rightarrow \\mathcal{Z}$, i.e., either modify feature representation or modify instances themself\n",
    "    * modify features $\\phi(x)$; we here could use composition here: $\\phi(x) \\circ \\phi'(x)$ to make it more explicit that $\\phi'$ might come from elsewhere than $\\mathrel{X}$\n",
    "        * add embeddings\n",
    "        * feature dropout\n",
    "        \n",
    "    * modify instances $X$:\n",
    "        * add instances:\n",
    "            * e.g., self-training, co-training \n",
    "            * projection in cross-lingual learning\n",
    "        * \"drop\" instances (importance weighting/data selection)\n",
    "* modify the second part: $\\zeta: (\\mathcal{Z}, \\theta) \\rightarrow \\mathcal{Y}$, i.e., the algorithm itself (can refer both to training and decoding): **nb. not sure if this is best way to present it; it's strictly speaking not the scoring function itself, but the learning of weights, which is part of the scoring function** (but easier to point to it as it is the second part of f(.))\n",
    "    * distant supervision (wiktionary constraints)\n",
    "    * ILP for cross-lingual learning\n",
    "    * modify the objective/add auxiliary loss as in multi-task learning (might also involve first part if additional data from distinct sources is included);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Challenges \n",
    "% these are just notes\n",
    "<s>* NLP: sparse data. Need for large amounts of labeled data (expensive)\n",
    "   * (e.g. self-training approaches)\n",
    "* Only little labeled data, but related data is available. how to incorporate?\n",
    "   * data from related resources (e.g. Wiktionary; type-constraints)\n",
    "   * data from related tasks (e.g. multitask-learning day 4)\n",
    "* Domain adaptation\n",
    "    * change in input representation (e.g. from news to twitter; example hyperlinks)\n",
    "    * change in relation between input and output (e.g., cross-lingual learning -> extreme case of not having any data available, day 5)</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The hunt for a learning signal \n",
    "\n",
    "### A learning analogy\n",
    "\n",
    "(cooking example) \n",
    "\n",
    "### Learning signal\n",
    "\n",
    "(comparing input output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification and gradient-based learning\n",
    "\n",
    "(day 1 classification, i.e., In the first lecture weâ€™ll assume that both our input and output have a fixed size and structure, i.e. the problem is classification;) e.g., [shelter outcome classification problem](https://www.kaggle.com/c/shelter-animal-outcomes)? <img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/5039/media/kaggle_pets2.png\">\n",
    "\n",
    "ubiquitous in NLP and accounts for most successes in machine learning, including deep learning\n",
    "\n",
    "(give intuition of gradient-based learning, more details in day 2?)\n",
    "\n",
    "ML as function learning (examples from your [SciProg class](https://github.com/andersjo/scientific-programming-2015/blob/master/lectures/lecture05/ML_Classification.ipynb) ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning from non-obvious sources: Fortuitous data\n",
    "\n",
    "<img src=\"pics/fortuitous-def.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
